{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pBQsZEJmubLs"
   },
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "\n",
    "# Neural Network Framework (Keras)\n",
    "\n",
    "## *Data Science Unit 4 Sprint 2 Assignment 3*\n",
    "\n",
    "## Use the Keras Library to build a Multi-Layer Perceptron Model on the Boston Housing dataset\n",
    "\n",
    "- The Boston Housing dataset comes with the Keras library so use Keras to import it into your notebook. \n",
    "- Normalize the data (all features should have roughly the same scale)\n",
    "- Import the type of model and layers that you will need from Keras.\n",
    "- Instantiate a model object and use `model.add()` to add layers to your model\n",
    "- Since this is a regression model you will have a single output node in the final layer.\n",
    "- Use activation functions that are appropriate for this task\n",
    "- Compile your model\n",
    "- Fit your model and report its accuracy in terms of Mean Squared Error\n",
    "- Use the history object that is returned from model.fit to make graphs of the model's loss or train/validation accuracies by epoch. \n",
    "- Run this same data through a linear regression model. Which achieves higher accuracy?\n",
    "- Do a little bit of feature engineering and see how that affects your neural network model. (you will need to change your model to accept more inputs)\n",
    "- After feature engineering, which model sees a greater accuracy boost due to the new features?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8NLTAR87uYJ-"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.datasets import boston_housing\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.layers import Flatten\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LOAD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = boston_housing.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((404, 13), (404,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>404.000000</td>\n",
       "      <td>404.000000</td>\n",
       "      <td>404.000000</td>\n",
       "      <td>404.000000</td>\n",
       "      <td>404.000000</td>\n",
       "      <td>404.000000</td>\n",
       "      <td>404.000000</td>\n",
       "      <td>404.000000</td>\n",
       "      <td>404.000000</td>\n",
       "      <td>404.000000</td>\n",
       "      <td>404.000000</td>\n",
       "      <td>404.000000</td>\n",
       "      <td>404.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.745111</td>\n",
       "      <td>11.480198</td>\n",
       "      <td>11.104431</td>\n",
       "      <td>0.061881</td>\n",
       "      <td>0.557356</td>\n",
       "      <td>6.267082</td>\n",
       "      <td>69.010644</td>\n",
       "      <td>3.740271</td>\n",
       "      <td>9.440594</td>\n",
       "      <td>405.898515</td>\n",
       "      <td>18.475990</td>\n",
       "      <td>354.783168</td>\n",
       "      <td>12.740817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>9.240734</td>\n",
       "      <td>23.767711</td>\n",
       "      <td>6.811308</td>\n",
       "      <td>0.241238</td>\n",
       "      <td>0.117293</td>\n",
       "      <td>0.709788</td>\n",
       "      <td>27.940665</td>\n",
       "      <td>2.030215</td>\n",
       "      <td>8.698360</td>\n",
       "      <td>166.374543</td>\n",
       "      <td>2.200382</td>\n",
       "      <td>94.111148</td>\n",
       "      <td>7.254545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.006320</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.460000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.385000</td>\n",
       "      <td>3.561000</td>\n",
       "      <td>2.900000</td>\n",
       "      <td>1.129600</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>188.000000</td>\n",
       "      <td>12.600000</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>1.730000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.081437</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.130000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.453000</td>\n",
       "      <td>5.874750</td>\n",
       "      <td>45.475000</td>\n",
       "      <td>2.077100</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>279.000000</td>\n",
       "      <td>17.225000</td>\n",
       "      <td>374.672500</td>\n",
       "      <td>6.890000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.268880</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.690000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.538000</td>\n",
       "      <td>6.198500</td>\n",
       "      <td>78.500000</td>\n",
       "      <td>3.142300</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>330.000000</td>\n",
       "      <td>19.100000</td>\n",
       "      <td>391.250000</td>\n",
       "      <td>11.395000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.674808</td>\n",
       "      <td>12.500000</td>\n",
       "      <td>18.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.631000</td>\n",
       "      <td>6.609000</td>\n",
       "      <td>94.100000</td>\n",
       "      <td>5.118000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>666.000000</td>\n",
       "      <td>20.200000</td>\n",
       "      <td>396.157500</td>\n",
       "      <td>17.092500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>88.976200</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>27.740000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.871000</td>\n",
       "      <td>8.725000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>10.710300</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>711.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>396.900000</td>\n",
       "      <td>37.970000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               0           1           2           3           4           5   \\\n",
       "count  404.000000  404.000000  404.000000  404.000000  404.000000  404.000000   \n",
       "mean     3.745111   11.480198   11.104431    0.061881    0.557356    6.267082   \n",
       "std      9.240734   23.767711    6.811308    0.241238    0.117293    0.709788   \n",
       "min      0.006320    0.000000    0.460000    0.000000    0.385000    3.561000   \n",
       "25%      0.081437    0.000000    5.130000    0.000000    0.453000    5.874750   \n",
       "50%      0.268880    0.000000    9.690000    0.000000    0.538000    6.198500   \n",
       "75%      3.674808   12.500000   18.100000    0.000000    0.631000    6.609000   \n",
       "max     88.976200  100.000000   27.740000    1.000000    0.871000    8.725000   \n",
       "\n",
       "               6           7           8           9           10          11  \\\n",
       "count  404.000000  404.000000  404.000000  404.000000  404.000000  404.000000   \n",
       "mean    69.010644    3.740271    9.440594  405.898515   18.475990  354.783168   \n",
       "std     27.940665    2.030215    8.698360  166.374543    2.200382   94.111148   \n",
       "min      2.900000    1.129600    1.000000  188.000000   12.600000    0.320000   \n",
       "25%     45.475000    2.077100    4.000000  279.000000   17.225000  374.672500   \n",
       "50%     78.500000    3.142300    5.000000  330.000000   19.100000  391.250000   \n",
       "75%     94.100000    5.118000   24.000000  666.000000   20.200000  396.157500   \n",
       "max    100.000000   10.710300   24.000000  711.000000   22.000000  396.900000   \n",
       "\n",
       "               12  \n",
       "count  404.000000  \n",
       "mean    12.740817  \n",
       "std      7.254545  \n",
       "min      1.730000  \n",
       "25%      6.890000  \n",
       "50%     11.395000  \n",
       "75%     17.092500  \n",
       "max     37.970000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(X_train).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>404.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>22.395050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>9.210442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>16.675000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>20.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>24.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>50.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                0\n",
       "count  404.000000\n",
       "mean    22.395050\n",
       "std      9.210442\n",
       "min      5.000000\n",
       "25%     16.675000\n",
       "50%     20.750000\n",
       "75%     24.800000\n",
       "max     50.000000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(y_train).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SCALE INPUT DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the input data\n",
    "scaler = StandardScaler()\n",
    "X_train_scld = scaler.fit_transform(X_train)\n",
    "X_test_scld  = scaler.fit_transform(X_test)\n",
    "\n",
    "# print(f'X_train scaled:\\n{pd.DataFrame(X_train_scld).describe()}')\n",
    "# print(f'\\nX_test scaled:\\n{pd.DataFrame(X_test_scld).describe()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STAND UP A NEURAL NETWORK MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 13)                182       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 14        \n",
      "=================================================================\n",
      "Total params: 196\n",
      "Trainable params: 196\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create a model object (to be configured)\n",
    "model = Sequential()\n",
    "\n",
    "# Declare input and hidden layers\n",
    "model.add(Dense(13, input_dim=13, activation='relu'))\n",
    "model.add(Dense(1, input_dim=13, activation='relu'))\n",
    "\n",
    "# Configure the model for fitting/training\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FIT/TRAIN THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "model_history = model.fit(X_train_scld, y_train, epochs=num_epochs, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PLOT MODEL HISTORY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x144200cc0>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO3deXwV9b3/8dfn5GQhCyQkIUAIe9gpIKhQUNncq2iLta2t3NaW3mrdey3e9i793VbrtVfUWm1tbWsXrVeqxapXBQSXutSEfU9AlgRIAoRAErJ/f3+c0UZlSSDJ5Ezez8djHjPznTk5n3kMvDP5njnfMeccIiISLCG/CxARkbancBcRCSCFu4hIACncRUQCSOEuIhJAYb8LAMjIyHADBw70uwwRkaiSn5+/3zmXeaxtnSLcBw4cSF5ent9liIhEFTPbebxt6pYREQkghbuISAAp3EVEAkjhLiISQAp3EZEAUriLiASQwl1EJICiOty3lVXy4//bjIYtFhH5qKgO9+WbS/n5a9t4/K0dfpciItKpRHW4f23qIGaN6MWPXtzEmt2H/C5HRKTTiOpwD4WM//n8OHqlJHDDEyupqK73uyQRkU4hqsMdIDUxjp9+aQL7Kmr4l0Vr1P8uIkIAwh3gjP5pLLh4BK9sLFH/u4gIAQl3gOumDWLmiF7c9eJmNuyp8LscERFfBSbczYx7536K1MRYbnxyFdV1DX6XJCLim8CEO0B6cjz3f2E87++v4j8Wb/C7HBER3wQq3AE+PSSDG6YP5en8Ip5bs8fvckREfBG4cAe4ZXYuE/qn8r1n11FUXu13OSIiHS6Q4R6OCXH/1eNpanLc9tQaGpt0e6SIdC2BDHeAAelJ/L85Y/j7joM8sqLQ73JERDpUYMMd4LNnZHPZuL4sXFqg4QlEpEsJdLibGT+8YgyZyfH8y6I11DY0+l2SiEiHaFG4m9kOM1tnZqvNLM9r62lmS8yswJunee1mZg+aWaGZrTWzM9rzAE6mR7dY7v7sWLaWVPLTZeqeEZGuoTVX7jOcc+Odc5O89QXAMudcLrDMWwe4GMj1pvnAI21V7KmaMaIXcyf245HXtrGuSN9eFZHgO51umTnA497y48AVzdp/5yLeAVLNrM9pvE+b+LdLR5GRHMd3nl5DXUOT3+WIiLSrloa7A14xs3wzm++1ZTnn9nrL+4Asbzkb2N3stUVe20eY2XwzyzOzvLKyslMovXV6JMZy15Vj2VJyhId194yIBFxLw32ac+4MIl0uN5jZuc03usg4u626mdw596hzbpJzblJmZmZrXnrKZo3M4rJxfXl4+Ta2lVV2yHuKiPihReHunCv25qXAs8BZQMkH3S3evNTbvRjIafbyfl5bp/BvnxlJQmyI7z27TmO/i0hgnTTczSzJzFI+WAYuANYDzwHzvN3mAYu95eeAa727ZiYDFc26b3zXKyWBBReP5J3tB3k6v8jvckRE2kVLrtyzgDfNbA3wd+AF59xLwI+B882sAJjtrQO8CGwHCoFfAte3edWn6Qtn5jBpQBp3vbiJA5W1fpcjItLmrDN0TUyaNMnl5eV16HsWlBzhkgffYM74bH5y1bgOfW8RkbZgZvnNbk//iEB/Q/VEcrNSuG7aYBblF5G346Df5YiItKkuG+4AN80aSt8eCXz/L+tpaNS97yISHF063BPjwvz7ZaPYvO8Ij7+90+9yRETaTJcOd4ALR/dm+vBMFi7ZSsnhGr/LERFpE10+3M2MH1w+mrrGJu5+cZPf5YiItIkuH+4QebDH/HMG85fVe8jfWe53OSIip03h7vnW9CFkdY/nB3/dQJMeyyciUU7h7kmKD7Pg4hGsLapg0Up9c1VEopvCvZkrxmdzRv9U/vulLRypqfe7HBGRU6Zwb8bM+I/LRrO/spafLd/mdzkiIqdM4f4x43JS+eyEbH79t/fZc+io3+WIiJwShfsx3Hr+MHCwcMlWv0sRETklCvdjyOmZyLVTBvDnlUVs2XfE73JERFpN4X4cN8wYSlJ8mP9+abPfpYiItJrC/TjSkuL41vQhLNtcyrvbD/hdjohIqyjcT+BrUwfRu3sCP35psx7JJyJRReF+AgmxMdwyO5dVuw6xZGOJ3+WIiLSYwv0k5k7sx+CMJO59eQuNGpZARKKEwv0kwjEhvnPhcApKK3l2VbHf5YiItIjCvQUuHtObsdk9WLhkK7UNjX6XIyJyUgr3FjAzvnvRCIoPHeUP7+zyuxwRkZNSuLfQtNwMpg5N52fLC6msbfC7HBGRE1K4t8K/XDiCg1V1PPbG+36XIiJyQgr3Vhifk8qFo7P45RvbOVhV53c5IiLHpXBvpe9cMJzqugYeWVHodykiIselcG+l3KwUrpzQj8ff3sneCg0JLCKdk8L9FNwyOxfnHA8sLfC7FBGRY1K4n4Kcnolcc/YAns4vYsf+Kr/LERH5BIX7Kbp++hBiY4wHl+nqXUQ6nxaHu5nFmNkqM3veWx9kZu+aWaGZPWVmcV57vLde6G0f2D6l+6tX9wTmTRnIs6uLKSjRAz1EpHNpzZX7zcCmZuv3AAudc0OBcuA6r/06oNxrX+jtF0jfPG8IibEx3K++dxHpZFoU7mbWD7gU+JW3bsBMYJG3y+PAFd7yHG8db/ssb//A6ZkUx3XTBvHCur1s3HPY73JERD7U0iv3+4E7gCZvPR045Jz74Hv4RUC2t5wN7Abwtld4+3+Emc03szwzyysrKzvF8v133TmD6Z4Q5r4lW/wuRUTkQycNdzP7DFDqnMtvyzd2zj3qnJvknJuUmZnZlj+6Q/XoFsv8cwezdFMpa4sO+V2OiAjQsiv3qcDlZrYD+BOR7pgHgFQzC3v79AM+GOy8GMgB8Lb3AAL9ENJ/mjqI1MRY7luy1e9SRESAFoS7c+5O51w/59xA4AvAq865a4DlwFxvt3nAYm/5OW8db/urLuAPIE2OD/PNc4ewYksZ+TvL/S5HROS07nP/LnCbmRUS6VN/zGt/DEj32m8DFpxeidHh2ikDSE+K4/6lunoXEf+FT77LPzjnVgArvOXtwFnH2KcGuKoNaosqSfFh/vm8IfzoxU38/f2DnDWop98liUgXpm+otqEvTx5ARnK87pwREd8p3NtQt7gYrp8+hHe2H+Stbfv9LkdEujCFexv70tn9yeoez8IlWwn458gi0okp3NtYQmwM354xlPd2lPNGga7eRcQfCvd28Pkzc+jbI4H7dPUuIj5RuLeD+HAMN87KZfXuQyzfUup3OSLSBSnc28ncif3I6dlNV+8i4guFezuJjQlx08xc1hcf5pWNJX6XIyJdjMK9HV05IZvBGUnc98pWmpp09S4iHUfh3o7CMSFunp3LlpIjvLBur9/liEgXonBvZ5d9qi/Ds1JYuHQrDY1NJ3+BiEgbULi3s1DIuPX8XLaXVbF49R6/yxGRLkLh3gEuHN2b0X2788CyAup19S4iHUDh3gHMjNsvGMaug9X8Ob/I73JEpAtQuHeQGcN7MT4nlQeXFVDb0Oh3OSIScAr3DmJmfOeC4eypqOGp93b7XY6IBJzCvQNNHZrOWYN68tCrhdTU6+pdRNqPwr0DmRm3nz+M0iO1/OGdnX6XIyIBpnDvYGcPTuec3AweXrGNqtoGv8sRkYBSuPvgtvOHcbCqjt++tcPvUkQkoBTuPpjQP41ZI3rxi9e2UXG03u9yRCSAFO4+ufX8YRyuaeCxN7b7XYqIBJDC3SdjsntwydjePPbm+xysqvO7HBEJGIW7j26dPYzq+kZ+8do2v0sRkYBRuPsoNyuFK8Zn8/jbOyg9XON3OSISIAp3n90yO5eGRsdPXy30uxQRCRCFu88GpCfx+TNzePLvu9h1oNrvckQkIBTuncBNM3OJCRn3L93qdykiEhAK906gd48E/unTA3l2dTFb9h3xuxwRCYCThruZJZjZ381sjZltMLMfeO2DzOxdMys0s6fMLM5rj/fWC73tA9v3EILhn88bQnJcmJ+8ssXvUkQkAFpy5V4LzHTOjQPGAxeZ2WTgHmChc24oUA5c5+1/HVDutS/09pOTSEuKY/65g1mysYT8neV+lyMiUe6k4e4iKr3VWG9ywExgkdf+OHCFtzzHW8fbPsvMrM0qDrCvTRtEZko8P3phI845v8sRkSjWoj53M4sxs9VAKbAE2AYccs59MKxhEZDtLWcDuwG87RVAelsWHVRJ8WG+c8EwVu46xIvr9vldjohEsRaFu3Ou0Tk3HugHnAWMON03NrP5ZpZnZnllZWWn++MCY+7EHEb0TuHHL23SAz1E5JS16m4Z59whYDkwBUg1s7C3qR9Q7C0XAzkA3vYewIFj/KxHnXOTnHOTMjMzT7H84IkJGd+/dBS7Dx7lcQ0JLCKnqCV3y2SaWaq33A04H9hEJOTnervNAxZ7y89563jbX3XqQG6VabkZzBzRi4deLeRAZa3f5YhIFGrJlXsfYLmZrQXeA5Y4554HvgvcZmaFRPrUH/P2fwxI99pvAxa0fdnB96+XjKC6vpH7lxb4XYqIRKHwyXZwzq0FJhyjfTuR/vePt9cAV7VJdV3Y0F4pfPns/vz+nZ18efIAhvdO8bskEYki+oZqJ3bL7GEkx4f5oW6NFJFWUrh3YmlJcdwyexhvFOxn+ZZSv8sRkSiicO/kvjJlAIMzk/jh85uob2zyuxwRiRIK904uNibE9y8dyfb9Vbo1UkRaTOEeBWYM78WM4ZksXLKVfRV6YpOInJzCPQqYGT+4fAwNTY7/en6j3+WISBRQuEeJ/umJfHvGUF5Yt5fXtmq4BhE5MYV7FJl/3mAGZyTx74vXa9wZETkhhXsUiQ/H8F9XjGHngWoeXrHN73JEpBNTuEeZqUMzuHxcX36+YhvbyipP/gIR6ZIU7lHo+58ZSXxsiO8/u17fXBWRY1K4R6FeKQl896IRvL39AM+uKj75C0Sky1G4R6kvndWfCf1T+dELmzhUXed3OSLSySjco1QoZNx15VgOHa3nhy9s8rscEelkFO5RbGSf7nzrvCEsyi/ilQ165qqI/IPCPcrdNCuX0X27c+cz69ivpzaJiEfhHuXiwiEWXj2eI7UNLPjzOt09IyKAwj0QhmWlcMeFw1m6qYT/zdvtdzki0gko3APia1MHMWVwOv/53EZ9uUlEFO5BEQoZC68eT0JsiBufWEVtg8aeEenKFO4B0rtHAvfOHcfGvYe5+8XNfpcjIj5SuAfM7FFZ/NOnB/Lbt3awdGOJ3+WIiE8U7gF05yUjGN23O7c/vYai8mq/yxERHyjcAyg+HMPPvnQGTU2OG/64Uv3vIl2Qwj2gBmYkce9V41hTVMGPNDyBSJejcA+wi8b05hvnDOJ3b+9k8WqNHinSlSjcA+6Oi0Zw5sA0vvvntawrqvC7HBHpIAr3gIuNCfHwNRNJT4rn6797j30VNX6XJCIdQOHeBWSmxPOreZOorGng6797j+q6Br9LEpF2pnDvIkb26c6DX5zAhj2HufWp1TQ2aYAxkSA7abibWY6ZLTezjWa2wcxu9tp7mtkSMyvw5mleu5nZg2ZWaGZrzeyM9j4IaZlZI7P4t0tH8fKGEv71GY0gKRJkLblybwBud86NAiYDN5jZKGABsMw5lwss89YBLgZyvWk+8EibVy2n7GvTBnHjzKE8lbebu/9vswJeJKDCJ9vBObcX2OstHzGzTUA2MAeY7u32OLAC+K7X/jsXSY13zCzVzPp4P0c6gdvOH0bF0XoefX07PbrFcsOMoX6XJCJt7KTh3pyZDQQmAO8CWc0Cex+Q5S1nA80HFS/y2j4S7mY2n8iVPf37929l2XI6zIz/vGw0h4/Wc+/LW0hJCHPtlIF+lyUibajFH6iaWTLwZ+AW59zh5tu8q/RW/X3vnHvUOTfJOTcpMzOzNS+VNhAKGfdeNY7ZI7P498UbWJRf5HdJItKGWhTuZhZLJNj/6Jx7xmsuMbM+3vY+QKnXXgzkNHt5P69NOpnYmBAPfWkCU4emc8eiNby0Xj1nIkHRkrtlDHgM2OScu6/ZpueAed7yPGBxs/ZrvbtmJgMV6m/vvBJiY3j0K5MYn5PKjU+u4q9r9vhdkoi0gZZcuU8FvgLMNLPV3nQJ8GPgfDMrAGZ76wAvAtuBQuCXwPVtX7a0paT4ML/56llMyEnjpj+t4vdv7/C7JBE5TdYZboWbNGmSy8vL87uMLq+mvpFvP7GSpZtKuXlWLrfMziXyh5uIdEZmlu+cm3SsbfqGqnwoITaGn395InMn9uOBZQXc+cw66hub/C5LRE5Bq26FlOALx4S4d+6n6NMjgZ++Wsjeihp+ds0ZJMfrn4pINNGVu3yCmXH7BcO5+7NjebNwP1f/4m2NJikSZRTuclxfPKs/v5o3iR37q7j8oTdZvfuQ3yWJSAsp3OWEZgzvxTPXTyUuHOLqX7ytJzqJRAmFu5zU8N4pLL5hKuNyUrn5T6v5r+c3UtegD1pFOjOFu7RIenI8f7jubOZNGcBjb77P1Y++TfGho36XJSLHoXCXFosLh/jBnDH87EtnUFBSyaUPvsErG/b5XZaIHIPCXVrt0k/14a83TiM7tRvzf5/Pnc+s1aP7RDoZhbuckkEZSTx7/VS+ed5g/vTebi598E1W7Sr3uywR8Sjc5ZTFhUPcefFInvj6ZGrrG/ncI29xz0ubqW1o9Ls0kS5P4S6nbcqQdF669VyumpjDIyu2cdlP32SN7okX8ZXCXdpE94RY7pn7KX7z1TOpOFrPlQ//jbte3MTROl3Fi/hB4S5tasbwXiy57TyuPrM/j76+nYseeJ03C/b7XZZIl6NwlzbXPSGWuz87lie+cTYAX37sXa7/Y77uixfpQAp3aTefHpLBy7ecy+3nD+PVzaXM+p8VPLisgJp6ddWItDeFu7SrhNgYbpyVy9LbzmPG8F7ct2Qrs/7nNf66Zg+d4UExIkGlcJcO0S8tkUe+PJEnvzGZHt1iufHJVXzukbfI23HQ79JEAknhLh1qypB0/nrjNO753FiKyo8y9+dv883f57GtrNLv0kQCRc9QFd9U1zXw2Bvv8/PXtlHT0MQXz8rh5lnDyEyJ97s0kahwomeoKtzFd/sra3lwWQFPvLuL+HCIb5w7mOumDSIlIdbv0kQ6NYW7RIXtZZXc+/IW/m/9PlITY/nn84Zw7ZQBJMbp+a0ix6Jwl6iytugQ9y3ZyootZWQkxzH/3MFcc/YAkvSQbpGPULhLVMrbcZAHlhXwRsF+eibF8fVzBnHtlIEkK+RFAIW7RLn8neU8uKyA17aWkZYYyzfOHcy8KQN1JS9dnsJdAmHVrnIeWFbAii2RkL92ykCunTKA9GTdXSNdk8JdAmXlrnIeXl7I0k2lxIdDXDWpH/PPGUL/9ES/SxPpUAp3CaTC0iP88vX3eXZVMY3O8ZlP9eFb04cwond3v0sT6RAKdwm0ksM1/OqN7fzx3V1U1zUyY3gm888dwuTBPTEzv8sTaTcnCveTDj9gZr82s1IzW9+sraeZLTGzAm+e5rWbmT1oZoVmttbMzmi7wxA5tqzuCXzv0lG8tWAmt50/jLVFFXzxl+9w+UN/4y+riqlraPK7RJEO15KxZX4LXPSxtgXAMudcLrDMWwe4GMj1pvnAI21TpsjJpSbGcdOsXP62YCZ3XTmWqtoGbnlqNdPueZWfLivgQGWt3yWKdJgWdcuY2UDgeefcGG99CzDdObfXzPoAK5xzw83sF97ykx/f70Q/X90y0h6amhyvFZTxm7/t4PWtZcSFQ1w5PpuvThuofnkJhBN1y5zqjcJZzQJ7H5DlLWcDu5vtV+S1fSLczWw+kat7+vfvf4pliBxfKGTMGN6LGcN7UVByhN+8tYNnVhbxVN5upgxO59opAzh/VBbhGA2OKsFz2v+qXeTSv9WfyjrnHnXOTXLOTcrMzDzdMkROKDcrhbuuHMvbC2Zxx0XD2XWwmm/9cSXn/PdyHnq1gP3qspGAOdVwL/G6Y/DmpV57MZDTbL9+XptIp5CWFMf104fy+h0zePQrExmSmcxPXtnKp+9+lVufWs3KXeV6QpQEwql2yzwHzAN+7M0XN2v/tpn9CTgbqDhZf7uIH2JCxgWje3PB6N4Ullby+7d3sCi/iGdXFTOyT3euObs/V0zI1jg2ErVO+oGqmT0JTAcygBLgP4C/AP8L9Ad2Ap93zh20yE3FDxG5u6Ya+Kpz7qSflOoDVekMKmsbWLy6mD+8s4tNew+TFBfD5eOzuebs/ozJ7uF3eSKfoC8xibSCc45Vuw/xxLu7eH7tHmrqmxiT3Z2rJuZw+bi+pCXF+V2iCKBwFzllFUfreXZlEU/nF7Fhz2HiYkLMHtWLuRP7cW5upu60EV8p3EXawMY9h1mUX8Ti1cUcqKojIzmeKyf05YoJ2Yzq011DHUiHU7iLtKH6xiZWbCnj6bzdLN9SSn2jY1hWMnPGZzNnfF/6pWl0SukYCneRdlJeVccL6/byl1XF5O0sB2DSgDTmjO/LRWP6kJmiseal/SjcRTrA7oPVPLdmD4tXF7O1pJKQweTB6Vwytg8XjM6iV0qC3yVKwCjcRTqQc44tJUd4ce1enl+3l+1lVZjBxP5pXDi6NxeO7q0Hi0ibULiL+OSDoH95fQkvb9jHxr2HARjZpzsXjs7iglG9GdknRR/GyilRuIt0ErsOVPPKxn28vGEfeTvLcQ769khg1sgspg/PZPLgdD34W1pM4S7SCe2vrOXVzaUs3VjCGwX7OVrfSGyMMXFAGtO90SyHZSXrql6OS+Eu0snVNjSSt6Oc1wvKeH3rfjZ53Td9eyQwLTeDqUMzmDIkXR/Kykco3EWizL6KGl7bWsryzWW8tW0/h2saABiWlcynh0TC/qxBPenRLdbnSsVPCneRKNbY5Niwp4I3C/fz9rYDvLfjIDX1kefC5vZKZuKANCYOSOPMgT0ZkJ6obpwuROEuEiC1DY2s3HmI/J0Hyd9Zzspdh6g4Wg9ARnIcEwekMT4njQn9Uxmb3UMf0AZYezxmT0R8Eh+OYcqQdKYMSQciz4rdVlbJezvKeW/HQVbuKuflDSUAmMHQzGTGZvdgjDeN6ttd49R3AbpyFwmgA5W1rCk6xJrdFawvrmBdcQWlRyKPEjSDAT0TGd47heFZKQzrncKwrBQGZSQRq1Euo4qu3EW6mPTkeGaOyGLmiKwP20oP17Bhz2HWF1ewYc9htpYeYcnGEpq867twyBiUkcTgzCQGZyYzOOMfc41hH30U7iJdRK/uCfTqnsCMEb0+bKupb6SwtJKC0iNsLamkoKSSgtJKlm0qpaHpH3/VpyXG0r9nIjne1C+tG/3SEslO7UZ2aje6xcX4cUhyAgp3kS4sITbmw7745uobm9h9sJr391fx/v4qtpVVUVRezbriCl5av+8jwQ+QmhhL7+4JZKd2o09qAn1Tu9G7ewK9UhLo1T2eXinx9OgWqzt5OpDCXUQ+ITYmFOmSyUz+xLbGJkfJ4RqKDx2lqLyaPYdq2FtxlL2HathTUUPezvIP79756M80MpLjvSmOjOR40pPj6ZkUS1piHD2T4kj15mmJsaQkxBIT0i+DU6VwF5FWiQkZfVO70Te1G2cO7HnMfapqGyg5XEPpkVpKj9Sy/0gtZZW1lB6u5UBVZHnT3iMcqKqlvvHYN3WYQUp8mB6JsaTEx9K9W5iUhFi6J/xjOSkuhsS4GBLjwiTFh0lJiMyT4mLoFhdDUlyYbnExxIdDXe6vBoW7iLS5pPjwca/8m3POUVXXSHlVHQer6iivjkwHq+qpOFrP4aP1HKqu40hNA0dqGth9sJojNQ0cPlrPkdqGFtcTMugWG0O3uDDd4kKR5dgYEmIjvwQSwjHEx4b+MY+N/EL4YB7/wTwcIi4mRFzYm2Ii2yJzbz0cIjYmRGw4RGyMERsKEfLhLxCFu4j4xsxIjg+THB8mp2frxrhvanIcrW+kuq6Ro3WNVNY2eFM91XWNVNc2UlXX8OH26rpGahoaqalr5Gi9N9U1crCqjpr6Rmobmj4xb6s7xUPGh78MPpiHYyLhf8vsYVw2rm/bvFEzCncRiUqhkEW6YNrpC1nOOeoam6ipb6LWC/u6xibqGrzJW65taPTmH22va2iioclR39jkTe7D/RqataUmts/4QAp3EZFjMDPiwzHEh2MgCgdo09fRREQCSOEuIhJACncRkQBSuIuIBJDCXUQkgBTuIiIBpHAXEQkghbuISAB1iicxmVkZsPMUX54B7G/DcqJFVzzurnjM0DWPuyseM7T+uAc45zKPtaFThPvpMLO84z1mKsi64nF3xWOGrnncXfGYoW2PW90yIiIBpHAXEQmgIIT7o34X4JOueNxd8Zihax53VzxmaMPjjvo+dxER+aQgXLmLiMjHKNxFRAIoqsPdzC4ysy1mVmhmC/yupz2YWY6ZLTezjWa2wcxu9tp7mtkSMyvw5ml+19rWzCzGzFaZ2fPe+iAze9c730+ZWZzfNbY1M0s1s0VmttnMNpnZlC5yrm/1/n2vN7MnzSwhaOfbzH5tZqVmtr5Z2zHPrUU86B37WjM7o7XvF7XhbmYxwM+Ai4FRwBfNbJS/VbWLBuB259woYDJwg3ecC4BlzrlcYJm3HjQ3A5uard8DLHTODQXKget8qap9PQC85JwbAYwjcvyBPtdmlg3cBExyzo0BYoAvELzz/Vvgoo+1He/cXgzketN84JHWvlnUhjtwFlDonNvunKsD/gTM8bmmNuec2+ucW+ktHyHynz2byLE+7u32OHCFPxW2DzPrB1wK/MpbN2AmsMjbJYjH3AM4F3gMwDlX55w7RMDPtScMdDOzMJAI7CVg59s59zpw8GPNxzu3c4DfuYh3gFQz69Oa94vmcM8GdjdbL/LaAsvMBgITgHeBLOfcXm/TPiDLp7Lay/3AHUCTt54OHHLONXjrQTzfg4Ay4Dded9SvzCyJgJ9r51wx8BNgF5FQrwDyCf75huOf29POt2gO9y7FzJKBPwO3OOcON9/mIvezBuaeVjP7DFDqnMv3u5YOFgbOAB5xzk0AqvhYF0zQzjWA1888h8gvt75AEp/svgi8tj630RzuxUBOs/V+XlvgmFkskWD/o3PuGa+55IM/07x5qb3jCw4AAAFaSURBVF/1tYOpwOVmtoNId9tMIn3Rqd6f7RDM810EFDnn3vXWFxEJ+yCfa4DZwPvOuTLnXD3wDJF/A0E/33D8c3va+RbN4f4ekOt9oh5H5AOY53yuqc15fc2PAZucc/c12/QcMM9bngcs7uja2otz7k7nXD/n3EAi5/VV59w1wHJgrrdboI4ZwDm3D9htZsO9plnARgJ8rj27gMlmluj9e//guAN9vj3HO7fPAdd6d81MBiqadd+0jHMuaifgEmArsA34nt/1tNMxTiPyp9paYLU3XUKkD3oZUAAsBXr6XWs7Hf904HlveTDwd6AQeBqI97u+djje8UCed77/AqR1hXMN/ADYDKwHfg/EB+18A08S+Uyhnshfadcd79wCRuRuwG3AOiJ3ErXq/TT8gIhIAEVzt4yIiByHwl1EJIAU7iIiAaRwFxEJIIW7iEgAKdxFRAJI4S4iEkD/HwQykFaqIqnrAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(range(num_epochs), model_history.history['loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SfcFnOONyuNm"
   },
   "source": [
    "## Use the Keras Library to build an image recognition network using the Fashion-MNIST dataset (also comes with keras)\n",
    "\n",
    "- Load and preprocess the image data similar to how we preprocessed the MNIST data in class.\n",
    "- Make sure to one-hot encode your category labels\n",
    "- The number of nodes in your output layer should equal the number of classes you want to predict for Fashion-MNIST.\n",
    "- Try different hyperparameters. What is the highest accuracy that you are able to achieve.\n",
    "- Use the history object that is returned from model.fit to make graphs of the model's loss or train/validation accuracies by epoch. \n",
    "- Remember that neural networks fall prey to randomness so you may need to run your model multiple times (or use Cross Validation) in order to tell if a change to a hyperparameter is truly producing better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IMPORTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LOAD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (60000, 28, 28)\n",
      "y shape: (60000,)\n"
     ]
    }
   ],
   "source": [
    "(XM_train, ym_train), (XM_test, ym_test) = mnist.load_data()\n",
    "\n",
    "print(f'X shape: {XM_train.shape}')\n",
    "print(f'y shape: {ym_train.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "szi6-IpuzaH1"
   },
   "outputs": [],
   "source": [
    "# Transform X values (0 => 255) to (0..1)\n",
    "XM_train_scld = XM_train.astype('float32') / 255.0\n",
    "XM_test_scld  = XM_test.astype('float32')  / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output is one of 10 classes: [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Encode the output as categorical data\n",
    "num_classes = 10 # number of classes we have is 10\n",
    "\n",
    "ym_train_cat = keras.utils.to_categorical(ym_train, num_classes)\n",
    "ym_test_cat  = keras.utils.to_categorical(ym_test, num_classes)\n",
    "print(f'Output is one of 10 classes: {ym_train_cat[2]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                7850      \n",
      "=================================================================\n",
      "Total params: 7,850\n",
      "Trainable params: 7,850\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "model_mnst = Sequential()\n",
    "model_mnst.add(Flatten(input_shape=(28, 28)))   # flatten the input into a single array\n",
    "model_mnst.add(Dense(10, activation=\"softmax\")) # add a layer with 20 outputs using the softmax activation function\n",
    "\n",
    "# Configure the model for training\n",
    "model_mnst.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_mnst.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.4695 - accuracy: 0.8770\n",
      "Epoch 2/100\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.3036 - accuracy: 0.9150\n",
      "Epoch 3/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.2837 - accuracy: 0.9209\n",
      "Epoch 4/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2734 - accuracy: 0.9233\n",
      "Epoch 5/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.2668 - accuracy: 0.9254\n",
      "Epoch 6/100\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.2622 - accuracy: 0.9278\n",
      "Epoch 7/100\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.2583 - accuracy: 0.9284\n",
      "Epoch 8/100\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.2555 - accuracy: 0.9286\n",
      "Epoch 9/100\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.2532 - accuracy: 0.9306\n",
      "Epoch 10/100\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.2509 - accuracy: 0.9310\n",
      "Epoch 11/100\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.2494 - accuracy: 0.9314\n",
      "Epoch 12/100\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.2476 - accuracy: 0.9316\n",
      "Epoch 13/100\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.2462 - accuracy: 0.9328\n",
      "Epoch 14/100\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.2449 - accuracy: 0.9324\n",
      "Epoch 15/100\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.2440 - accuracy: 0.9323\n",
      "Epoch 16/100\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.2431 - accuracy: 0.9328\n",
      "Epoch 17/100\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.2423 - accuracy: 0.9334\n",
      "Epoch 18/100\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.2411 - accuracy: 0.9340\n",
      "Epoch 19/100\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.2405 - accuracy: 0.9338\n",
      "Epoch 20/100\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.2396 - accuracy: 0.9343\n",
      "Epoch 21/100\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.2386 - accuracy: 0.9345\n",
      "Epoch 22/100\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.2386 - accuracy: 0.9344\n",
      "Epoch 23/100\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.2372 - accuracy: 0.9341\n",
      "Epoch 24/100\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.2374 - accuracy: 0.9341\n",
      "Epoch 25/100\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.2363 - accuracy: 0.9344\n",
      "Epoch 26/100\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.2359 - accuracy: 0.9352\n",
      "Epoch 27/100\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.2355 - accuracy: 0.9345\n",
      "Epoch 28/100\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.2351 - accuracy: 0.9354\n",
      "Epoch 29/100\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.2348 - accuracy: 0.9352\n",
      "Epoch 30/100\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.2341 - accuracy: 0.9357\n",
      "Epoch 31/100\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.2339 - accuracy: 0.9348\n",
      "Epoch 32/100\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.2334 - accuracy: 0.9353\n",
      "Epoch 33/100\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.2326 - accuracy: 0.9358\n",
      "Epoch 34/100\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.2331 - accuracy: 0.9363\n",
      "Epoch 35/100\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.2323 - accuracy: 0.9354\n",
      "Epoch 36/100\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.2323 - accuracy: 0.9357\n",
      "Epoch 37/100\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.2320 - accuracy: 0.9362\n",
      "Epoch 38/100\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.2314 - accuracy: 0.9362\n",
      "Epoch 39/100\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.2310 - accuracy: 0.9361\n",
      "Epoch 40/100\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.2313 - accuracy: 0.9364\n",
      "Epoch 41/100\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.2304 - accuracy: 0.9363\n",
      "Epoch 42/100\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.2299 - accuracy: 0.9361\n",
      "Epoch 43/100\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.2301 - accuracy: 0.9366\n",
      "Epoch 44/100\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.2295 - accuracy: 0.9363\n",
      "Epoch 45/100\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.2298 - accuracy: 0.9367\n",
      "Epoch 46/100\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.2293 - accuracy: 0.9358\n",
      "Epoch 47/100\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.2293 - accuracy: 0.9360\n",
      "Epoch 48/100\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.2289 - accuracy: 0.9366\n",
      "Epoch 49/100\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.2287 - accuracy: 0.9364\n",
      "Epoch 50/100\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.2286 - accuracy: 0.9369\n",
      "Epoch 51/100\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.2282 - accuracy: 0.9368\n",
      "Epoch 52/100\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.2282 - accuracy: 0.9364\n",
      "Epoch 53/100\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.2280 - accuracy: 0.9365\n",
      "Epoch 54/100\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.2275 - accuracy: 0.9371\n",
      "Epoch 55/100\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.2275 - accuracy: 0.9371\n",
      "Epoch 56/100\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.2272 - accuracy: 0.9368\n",
      "Epoch 57/100\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.2273 - accuracy: 0.9373\n",
      "Epoch 58/100\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.2271 - accuracy: 0.9371\n",
      "Epoch 59/100\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.2266 - accuracy: 0.9371\n",
      "Epoch 60/100\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.2268 - accuracy: 0.9366: \n",
      "Epoch 61/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.2264 - accuracy: 0.9370\n",
      "Epoch 62/100\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.2264 - accuracy: 0.9370\n",
      "Epoch 63/100\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.2263 - accuracy: 0.9369\n",
      "Epoch 64/100\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.2261 - accuracy: 0.9374\n",
      "Epoch 65/100\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.2261 - accuracy: 0.9373\n",
      "Epoch 66/100\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.2255 - accuracy: 0.9370\n",
      "Epoch 67/100\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.2256 - accuracy: 0.9373\n",
      "Epoch 68/100\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.2250 - accuracy: 0.9375\n",
      "Epoch 69/100\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.2252 - accuracy: 0.9377\n",
      "Epoch 70/100\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.2254 - accuracy: 0.9373\n",
      "Epoch 71/100\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.2250 - accuracy: 0.9373\n",
      "Epoch 72/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.2248 - accuracy: 0.9370\n",
      "Epoch 73/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.2249 - accuracy: 0.9372\n",
      "Epoch 74/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.2246 - accuracy: 0.9375\n",
      "Epoch 75/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.2243 - accuracy: 0.9377\n",
      "Epoch 76/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.2243 - accuracy: 0.9380\n",
      "Epoch 77/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2240 - accuracy: 0.9372\n",
      "Epoch 78/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.2239 - accuracy: 0.9380\n",
      "Epoch 79/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.2240 - accuracy: 0.9378\n",
      "Epoch 80/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.2237 - accuracy: 0.9379\n",
      "Epoch 81/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.2242 - accuracy: 0.9377: 0s - loss: 0.2\n",
      "Epoch 82/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.2236 - accuracy: 0.9376\n",
      "Epoch 83/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.2233 - accuracy: 0.9379\n",
      "Epoch 84/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.2232 - accuracy: 0.9378\n",
      "Epoch 85/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.2233 - accuracy: 0.9378\n",
      "Epoch 86/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2232 - accuracy: 0.9378\n",
      "Epoch 87/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2230 - accuracy: 0.9377\n",
      "Epoch 88/100\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.2229 - accuracy: 0.9380\n",
      "Epoch 89/100\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.2231 - accuracy: 0.9379\n",
      "Epoch 90/100\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.2228 - accuracy: 0.9381\n",
      "Epoch 91/100\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.2229 - accuracy: 0.9373\n",
      "Epoch 92/100\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.2228 - accuracy: 0.9380\n",
      "Epoch 93/100\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.2224 - accuracy: 0.9380\n",
      "Epoch 94/100\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.2227 - accuracy: 0.9376\n",
      "Epoch 95/100\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.2221 - accuracy: 0.9379\n",
      "Epoch 96/100\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.2224 - accuracy: 0.9379\n",
      "Epoch 97/100\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.2225 - accuracy: 0.9382\n",
      "Epoch 98/100\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.2222 - accuracy: 0.9382\n",
      "Epoch 99/100\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.2219 - accuracy: 0.9384\n",
      "Epoch 100/100\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.2218 - accuracy: 0.9384\n"
     ]
    }
   ],
   "source": [
    "model_mnst_num_epochs = 100\n",
    "\n",
    "model_mnst_history = model_mnst.fit(XM_train_scld, ym_train_cat, epochs=model_mnst_num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1442fae10>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAdUUlEQVR4nO3de5BcZ3nn8e9z+jbTc79Zl5HkkWxhI9msZeQbhEsAV2TYtVngD7NhcSgohy1cOIGqxRRZKusUVQu1OGy2DF5vMNcQJ4tJ0BIHBxzshcQ2Hvku2bIl2bJulkaaGY00t749+0efGfXMaKSRNKOW3v59qqY05/Q53e/xGf/67ed9z2lzd0REJFxRtRsgIiILS0EvIhI4Bb2ISOAU9CIigVPQi4gELlntBkzX2dnpPT091W6GiMh5ZdOmTQfdvet4j51zQd/T00Nvb2+1myEicl4xs52zPabSjYhI4BT0IiKBU9CLiAROQS8iEjgFvYhI4BT0IiKBU9CLiAQumKAfHi9w1y9e5unXB6rdFBGRc0owQT9eKPEXD7/Cs7sGq90UEZFzSjBBn0oYAPmivkhFRKRSQEFfPpRcsVTlloiInFuCC/qCevQiIlMEE/SJyEhERl49ehGRKYIJeijX6RX0IiJTBRb0kWr0IiLTBBX06USkHr2IyDRBBX0qEZEvaDBWRKRSWEGfVI1eRGS6sIJeNXoRkRmCCnrV6EVEZgoq6FOJSLdAEBGZZk5Bb2YbzGyrmW0zsztOsN2HzczNbH283GNmo2b2TPxzz3w1/Hg0j15EZKbkyTYwswRwN3A9sBt40sw2uvuWads1AbcDT0x7iu3ufsU8tfeEUomIXEFBLyJSaS49+quBbe6+w91zwP3ATcfZ7s+ArwJj89i+U5JSjV5EZIa5BH03sKtieXe8bpKZXQksd/d/OM7+K83saTN71MzecbwXMLNbzazXzHr7+vrm2vYZyqUb1ehFRCqd8WCsmUXAXcDnj/PwPmCFu68DPgf8yMyap2/k7ve6+3p3X9/V1XXabVGPXkRkprkE/R5gecXysnjdhCbgMuARM3sNuBbYaGbr3X3c3Q8BuPsmYDvwpvlo+PGkkppHLyIy3VyC/klgtZmtNLM0cDOwceJBdz/s7p3u3uPuPcDjwI3u3mtmXfFgLma2ClgN7Jj3o4hpHr2IyEwnnXXj7gUzuw14CEgA97n7ZjO7E+h1940n2P2dwJ1mlgdKwKfdvX8+Gn48qYTpXjciItOcNOgB3P1B4MFp6748y7bvrvj9AeCBM2jfKUklIgol9ehFRCoFd2Ws5tGLiEwVVNCnk7oFgojIdEEFvW6BICIyU2BBH1EoOaWSevUiIhOCC3qAvAZkRUQmBRX06YmgV51eRGRSUEGfShgAec28ERGZFFbQJyd69Ap6EZEJYQV9XLrR/W5ERI4JKuhVoxcRmSmooJ+cdaMevYjIpKCCPhkPxuo2CCIixwQV9Gn16EVEZggq6FOq0YuIzBBY0Mfz6NWjFxGZFFbQJzW9UkRkuqCCfrJGr8FYEZFJQQW9avQiIjMFFvTlGr2+TlBE5JjAgj6u0at0IyIyKaigTydVuhERmS6ooNctEEREZgos6DWPXkRkusCCXvPoRUSmCzLo8wXV6EVEJgQV9InISESm0o2ISIWggh7KdXoFvYjIMQEGfaQavYhIheCCPp2I1KMXEakQXNAnE6bBWBGRCsEFfUo9ehGRKeYU9Ga2wcy2mtk2M7vjBNt92MzczNZXrPtivN9WM/u9+Wj0iaRVoxcRmSJ5sg3MLAHcDVwP7AaeNLON7r5l2nZNwO3AExXr1gA3A2uBpcAvzexN7l6cv0OYSj16EZGp5tKjvxrY5u473D0H3A/cdJzt/gz4KjBWse4m4H53H3f3V4Ft8fMtmFTSdFMzEZEKcwn6bmBXxfLueN0kM7sSWO7u/3Cq+8b732pmvWbW29fXN6eGz0Y9ehGRqc54MNbMIuAu4POn+xzufq+7r3f39V1dXWfUnlQi0v3oRUQqnLRGD+wBllcsL4vXTWgCLgMeMTOAxcBGM7txDvvOu3QiYjS/YEMAIiLnnbn06J8EVpvZSjNLUx5c3TjxoLsfdvdOd+9x9x7gceBGd++Nt7vZzDJmthJYDfx23o+igm6BICIy1Ul79O5eMLPbgIeABHCfu282szuBXnffeIJ9N5vZ3wJbgALwmYWccQMq3YiITDeX0g3u/iDw4LR1X55l23dPW/4K8JXTbN8pSyU1GCsiUim4K2PL97rR9EoRkQnBBb1q9CIiUwUY9CrdiIhUCjLoNRgrInJMcEGfTqpGLyJSKbigV41eRGSq4II+GUUUSk6ppF69iAgEGPTpZPmQ8iX16kVEIMCgTyUMQHV6EZFYgEEf9+g180ZEBAg56DUgKyICBBj06Tjo9b2xIiJlwQV9KqkavYhIpfCCXqUbEZEpgg163QZBRKQsuKCfqNEXdMGUiAgQYNCrdCMiMlWAQR8Pxqp0IyIChBj0SU2vFBGpFFzQpydLN6rRi4hAgEGvGr2IyFQBBv3EBVMKehERCDLoNY9eRKRScEE/eT961ehFRIAAgz4ZqXQjIlIpuKBPJTUYKyJSKbig122KRUSmCi7oj33DlGr0IiIQYNAnIiMylW5ERCYEF/RQ7tUr6EVEyuYU9Ga2wcy2mtk2M7vjOI9/2syeN7NnzOw3ZrYmXt9jZqPx+mfM7J75PoDjSSci1ehFRGLJk21gZgngbuB6YDfwpJltdPctFZv9yN3vibe/EbgL2BA/tt3dr5jfZp9YKqkevYjIhLn06K8Gtrn7DnfPAfcDN1Vu4O5DFYsNQFVHQlMJ02CsiEhsLkHfDeyqWN4dr5vCzD5jZtuBrwGfrXhopZk9bWaPmtk7zqi1c5RKRORL6tGLiMA8Dsa6+93ufhHwBeBP4tX7gBXuvg74HPAjM2uevq+Z3WpmvWbW29fXd8ZtSSci3QJBRCQ2l6DfAyyvWF4Wr5vN/cAHAdx93N0Pxb9vArYDb5q+g7vf6+7r3X19V1fXXNs+q1Qi0jdMiYjE5hL0TwKrzWylmaWBm4GNlRuY2eqKxQ8Ar8Tru+LBXMxsFbAa2DEfDT+RVNI0GCsiEjvprBt3L5jZbcBDQAK4z903m9mdQK+7bwRuM7P3AXlgALgl3v2dwJ1mlgdKwKfdvX8hDqRSStMrRUQmnTToAdz9QeDBaeu+XPH77bPs9wDwwJk08HTogikRkWOCvDJWg7EiIscEGfSphGr0IiITggz6ZCLSVwmKiMSCDPq0avQiIpOCDPpy6UY1ehERCDbo1aMXEZkQZtDr7pUiIpOCDPq0BmNFRCYFGfSq0YuIHBNo0Kt0IyIyIdigL5ScUkm9ehGRIIM+nSwflr58REQk0KBPJQxAdXoREYIN+vJhFVSnFxEJO+h1T3oRkUCDPh0HvUo3IiKBBn0qGdfoddGUiEigQT/Zo1fQi4gEHfSq0YuIBBr0qtGLiBwTZNAnJ+fRq0cvIhJk0E/W6DUYKyISdtCrRi8iEmjQq0YvInJMkEE/OY9ePXoRkUCDXvPoRUQmBRn0E6UbfZ2giEigQZ9SjV5EZFKgQa8avYjIhDCDPqkavYjIhCCDPq159CIik+YU9Ga2wcy2mtk2M7vjOI9/2syeN7NnzOw3Zram4rEvxvttNbPfm8/GzyaViDCDkfHi2Xg5EZFz2kmD3swSwN3ADcAa4KOVQR77kbtf7u5XAF8D7or3XQPcDKwFNgDfjJ9vQSUiY2VHAy/vP7LQLyUics6bS4/+amCbu+9w9xxwP3BT5QbuPlSx2ABMTHe5Cbjf3cfd/VVgW/x8C25tdwub9w6dfEMRkcDNJei7gV0Vy7vjdVOY2WfMbDvlHv1nT3HfW82s18x6+/r65tr2E7psaTN7BkcZGM7Ny/OJiJyv5m0w1t3vdveLgC8Af3KK+97r7uvdfX1XV9e8tGft0hYA9epFpObNJej3AMsrlpfF62ZzP/DB09x33qxd2gzA5r2Hz8bLiYics+YS9E8Cq81spZmlKQ+ubqzcwMxWVyx+AHgl/n0jcLOZZcxsJbAa+O2ZN/vk2hrSdLfW84J69CJS45In28DdC2Z2G/AQkADuc/fNZnYn0OvuG4HbzOx9QB4YAG6J991sZn8LbAEKwGfc/azNeVy7tFk9ehGpeScNegB3fxB4cNq6L1f8fvsJ9v0K8JXTbeCZWLu0hV+8uJ+j4wUaM3M6VBGR4AR5ZeyEy7qbcYcX96l8IyK1K+ign5x5s0flGxGpXUEH/aLmDJ2NaQ3IikhNCzrozYy1S3WFrIjUtqCDHsozb17Zf4Txgm5wJiK1Kfigv6y7hULJefmNo9VuiohIVQQf9BNXyL6g+fQiUqOCD/oV7Vla6lM8seNQtZsiIlIVwQe9mXHDZYt5aHP5wikRkVoTfNADfOStyxjNF/nH5/dVuykiImddTQT9Wy9so6cjywNP7a52U0REzrqaCHoz40NXLuPxHf3s6h+pdnNERM6qmgh6gA9dWf5iq588dVZuhy8ics6omaBf1pblulUd/OTp3bj7yXcQEQlEzQQ9wIffuoydh0bo3TlQ7aaIiJw1NRX0N1y2mGw6wQ8f31ntpoiInDU1FfQNmSQfv66Hnz6zl6dfV69eRGpDTQU9wG3vuZgLmjL86cbNlEqq1YtI+Gou6BszSe644VKe3X2YH2tevYjUgJoLeoAPXtHNlSta+drPtzI0lq92c0REFlRNBn0UGX9641oODY/z9Ye2Vrs5IiILqiaDHuAty1q55boevvfYTh7YpBKOiISrZoMe4EsfeDPXrergiz95nk07+6vdHBGRBVHTQZ9KRHzz969kSWsdf/iDTewZHK12k0RE5l1NBz1AW0Oab9+ynvF8iT+477ccPDpe7SaJiMyrmg96gIsvaOLej69n18AIH733cfqOKOxFJBwK+th1F3Xw3U9cze6BUW6+9zEODI1Vu0kiIvNCQV/h2lUdfPcTV7Hv8BgfuecxXtijLxQXkfOfgn6aa1Z18MNPXUOuUOJD3/xXfvDYa7qtsYic1xT0x3HlijYevP0dvP3iDv7LTzfz6R9uYt9hzcgRkfOTgn4W7Q1pvn3LVXzxhkv51dY+3vv1R/nWI9sZLxSr3TQRkVMyp6A3sw1mttXMtpnZHcd5/HNmtsXMnjOzh83sworHimb2TPyzcT4bv9CiyPjDd13EL//4Xbz94k6++vOX2PCNX/PI1gPVbpqIyJydNOjNLAHcDdwArAE+amZrpm32NLDe3d8C/Bj4WsVjo+5+Rfxz4zy1+6xa0ZHlf398Pd/5xFUA/MF3nuRT3+tl56HhKrdMROTk5tKjvxrY5u473D0H3A/cVLmBu//K3UfixceBZfPbzHPD715yAT//o3fwhQ2X8q/bD/Lerz/Krd/v5Zdb9lMolqrdPBGR40rOYZtuYFfF8m7gmhNs/0ngHyuW68ysFygA/83d/376DmZ2K3ArwIoVK+bQpOrJJBP8p3dfxL9f1813/uVVHnhqD/+0ZT8XNGX4xNtX8rFrV9BUl6p2M0VEJtnJpg6a2UeADe7+qXj5PwLXuPttx9n2Y8BtwLvcfTxe1+3ue8xsFfDPwHvdfftsr7d+/Xrv7e097QM62/LFEo9s7eP7j73Gr185SFNdko9deyE3X7WcCzsaqt08EakRZrbJ3dcf77G59Oj3AMsrlpfF66a/yPuAL1ER8gDuvif+d4eZPQKsA2YN+vNNKhFx/ZpFXL9mEc/vPsw9j27nnke3861HtnPlilY+uK6b91++hM7GTLWbKiI1ai49+iTwMvBeygH/JPAf3H1zxTbrKA/CbnD3VyrWtwEj7j5uZp3AY8BN7r5lttc733r0x7N3cJSNz+7l757aw9b9R0hExtsu6uDfvWUpb1/dydKWOsys2s0UkYCcqEd/0qCPn+D9wDeABHCfu3/FzO4Eet19o5n9Ergc2Bfv8rq732hmbwP+F1CiPPD7DXf/9oleK4Sgr/TiviF+9txe/u+z+3i9vzxevag5w7rlbVyzqp23XdTJmxY1KvhF5IyccdCfTaEF/QR3Z8u+IXpfG+Cp1wfYtHOA3QPlq207G9NcsbyNy7tbuHxZM2+9sJ2Weg3oisjcnWmNXuaBmbF2aQtrl7Zwy9t6ANjVP8JjOw7x+I5DPLtrkIdf2o87JCPj6pXtXL9mEdeu6mBlZwN1qUR1D0BEzlvq0Z9DhscLvLDnMI++3Mc/bdnPtgNHAYgMlrdnWbu0mat62rmqp503L2kmEancIyJlKt2cp149OMzzew6z7cBRth04wrO7Dk9+3WFTJsnVK9u5ZlU59JNRRDJhdDSkWdXVWOWWi8jZptLNeWplZwMrO6fOxd87OMpvX+3niVf7eWLHIR5+aeZ9d968pJkPrevmdy/tYixfYmg0T9GddSvaaMzolIvUGvXoz3P7h8bYeWiEYskplpxtB47wd8/s5dldgzO2TSWMt17Yxu9c3Mny9iyLm+tY3FL+ySQ1BiByPlPppgZtO3CUZ3YN0phJ0lKfolAq8S/bDvHoy328uG9oxvadjWmWtNSzuKWOpS11LG6ppy2bork+RXNdip7OLN2t9ZoGKnKOUtDLFEfG8rxxeIw3hsbYd3iMfYNjvDE0yt7BMfYdHmXf4TGOjBVm7NdSn2LNkmaWtNbRXJeiuS5JV3Mdy9rqWd6WZUV7lnRSX3EgUg2q0csUTXUpmupSrF7UNOs2w+MFBkfzHBnLMzCcZ3vfUTbvHWLLviGe2NHP0Fieo+MFKvsJ6UTEJYubuHxZC811KXYPjLBrYBR3Z+3SZtYubWFlZwNmYBjppNHekKGjMU1TJqlPCyILRD16OW2lktN3dJxd/SPsGhhh6xtHeW73IM/vOcxYvsjS1nJPv+TO5r1DHB7Nz/pcmWTEhR1Zejoa6OlsYFFzHYuaM7Rn0wznigyM5BgZL3DJ4mbWrWjVdQUi06hHLwsiiiwO5DrW97RPrnd33MuPV67bPTDKnsHRyU8BuWKJQ0fHOXQ0xxtDY+w8NMyOg8M88nIfucLs9/dPJyLesqyFVV0NLG2tZ0lLHYYxVigyni/Rmk3R09nAhe1ZosgYHMkxOJInm07S05klm9afvdQW/cXLvDMzpldhzIzl7VmWt2dPur+7MziSZ/+RMfqHczRlUrRmU2RSES/sOcwTO/rp3TnAI1v76Ds6zql+KF3SUkdbNk3JnZI76WREZ2OGzsYM7Q1pGtJJGjIJGjJJMsmITDJBNpNgUVP8KaMhrTKTnFcU9HLOMTPaGtK0NaRnPPaeS+t4z6WLJpdzhRIHjowBUJdKkElGHDqa47VDw5M3kWupT9GaTXN0rMCrB4+yo2+YobE8kRmJyBjLFzl4NMfWN44wMJJjLH/ibwurvCA5MqOxrjyzqbU+xeKWOrpbsyxpqSOZMErxm1BbNsUFTXVc0JwhMiNXKJEvlkglIhoz5TeW1mxaVzvLglDQy3ktnYxY1jb1U0JTXbl0c7oKxRLD40WGcwVyhRLjhRJHxwscGCrPVOofzuEOZlAsOUfGChwezTMwkmNH3zC/fuUgI7niKb9uKmGT4xr16QTD4wWGxwtEkU1+4mipT5GMym9QychIJiJSCSOdjKhLJahPJahLJTDK7UslIla0Z1nWVk8yoRlRtUpBLzJNMhHRko1oyZ7eHUTdnaGxAu6OUe6h94/kODA0xoEj45TcySQjklFEvlh+ExkeL7D/yMTA9igHj47TmEnSmk1TLDm7+kd4aucAQ2N5iiWf/KQw52OKym8iZpAvlCg5dLfVc3FXIyu7ym+KI+MFhnNFRnIFRnNFRvNFGjLJ8ieRpgyt2RQNmSQN6SRmMB6PiSQTEZ2N6ck3o/q0BsrPNQp6kXlmZjNuM92STc24ncWZcHcK8dXQ+WKJXKHEaL7IWL7IWL6EO5TcGcsXeb1/hFcPDrN7YJQo7uU75bunPvzSfg725iafN5tOkE0nyaYT1KUijo4V6Ds6Tr4493eWulREezZNQyZJqWJgfqK81Vyfil8nQToZMZorMRJ/emrNpulsStNSn2L/UPmNb+/gKBc017Gqs4FVXQ0saakvv7E0ZahLJkhERmTgDvlSiULRSURGJhlpLCWmoBc5D5kZqYSRSnDSqabXrOo44eNH4vGK+lRiykypCaWSMziaZ2g0P/npAyCTKr8ZjOdLHDw6Hv/kGBzJ0T+cZyRXIDIjioxCscTQWJ43hsbYuv8Io7kiI7ki44Xi5BtLKhExOJJjOC57Rcbk1drP7BrgZ8/tPeWB9/JgermsNTGGk45/3Jk8nkLJacuWx3Ka68qxOPGpKbJj/707GzMsaq6jo6E87XdoNM/weIHFLXVc1NXIhR1ZCqXyZILBkRzjhRKF+M0nm07S1pCivSFNW7b8Zna2pgkr6EVqXFPdiUtUUWS0N6RpP87g+EIYzRUZHM3R0ZCZcqX1WL7IzkMjHDgyxsF4Wu54oRyiRffJTyvJyCi6M5YvMR5/yhkvlCb/zRVK5IrlAfelrXU0ZpJEZgyO5OkfybF3cKx8UV/8nlf+dAS5QpHfHDnI0LSrxjPJiPETTAc+kfrUsdld6WTEZd0t/M+Prju9/3AnoKAXkXNKfTpBfbp+xvq6VIJLFjdxyeLZr+g+G0ZzRfpHcjSkEzTVpYgM+odzvHpwmJ2HRkgnI1qzKVrr09SnIxJR+c1nOFdgYDhP/3COwdHytR0Dw+VPMBNvPsvbZh73fFDQi4icgvp0gu5pb0QdjRk6GjNTLhw8l2i+lYhI4BT0IiKBU9CLiAROQS8iEjgFvYhI4BT0IiKBU9CLiAROQS8iErhz7qsEzawP2HkGT9EJHJyn5pwvavGYoTaPuxaPGWrzuE/1mC90967jPXDOBf2ZMrPe2b43MVS1eMxQm8ddi8cMtXnc83nMKt2IiAROQS8iErgQg/7eajegCmrxmKE2j7sWjxlq87jn7ZiDq9GLiMhUIfboRUSkgoJeRCRwwQS9mW0ws61mts3M7qh2exaKmS03s1+Z2RYz22xmt8fr283sF2b2SvxvW7XbOt/MLGFmT5vZz+LllWb2RHzO/8bMzs533Z1FZtZqZj82s5fM7EUzuy70c21mfxz/bb9gZn9tZnUhnmszu8/MDpjZCxXrjnturewv4uN/zsyuPJXXCiLozSwB3A3cAKwBPmpma6rbqgVTAD7v7muAa4HPxMd6B/Cwu68GHo6XQ3M78GLF8leBP3f3i4EB4JNVadXC+h/Az939UuDfUD7+YM+1mXUDnwXWu/tlQAK4mTDP9XeBDdPWzXZubwBWxz+3At86lRcKIuiBq4Ft7r7D3XPA/cBNVW7TgnD3fe7+VPz7Ecr/43dTPt7vxZt9D/hgdVq4MMxsGfAB4C/jZQPeA/w43iTEY24B3gl8G8Ddc+4+SODnmvJXnNabWRLIAvsI8Fy7+/8D+qetnu3c3gR838seB1rNbMlcXyuUoO8GdlUs747XBc3MeoB1wBPAInffFz/0BrCoSs1aKN8A/jNQipc7gEF3L8TLIZ7zlUAf8J24ZPWXZtZAwOfa3fcA/x14nXLAHwY2Ef65njDbuT2jjAsl6GuOmTUCDwB/5O5DlY95ec5sMPNmzezfAgfcfVO123KWJYErgW+5+zpgmGllmgDPdRvl3utKYCnQwMzyRk2Yz3MbStDvAZZXLC+L1wXJzFKUQ/6v3P0n8er9Ex/l4n8PVKt9C+DtwI1m9hrlstx7KNeuW+OP9xDmOd8N7Hb3J+LlH1MO/pDP9fuAV929z93zwE8on//Qz/WE2c7tGWVcKEH/JLA6HplPUx682VjlNi2IuDb9beBFd7+r4qGNwC3x77cAPz3bbVso7v5Fd1/m7j2Uz+0/u/vvA78CPhJvFtQxA7j7G8AuM7skXvVeYAsBn2vKJZtrzSwb/61PHHPQ57rCbOd2I/DxePbNtcDhihLPybl7ED/A+4GXge3Al6rdngU8zt+h/HHuOeCZ+Of9lGvWDwOvAL8E2qvd1gU6/ncDP4t/XwX8FtgG/B8gU+32LcDxXgH0xuf774G20M818F+Bl4AXgB8AmRDPNfDXlMch8pQ/vX1ytnMLGOWZhduB5ynPSprza+kWCCIigQuldCMiIrNQ0IuIBE5BLyISOAW9iEjgFPQiIoFT0IuIBE5BLyISuP8PkxWFBs45weMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(range(model_mnst_num_epochs), model_mnst_history.history['loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zv_3xNMjzdLI"
   },
   "source": [
    "## Stretch Goals:\n",
    "\n",
    "- Use Hyperparameter Tuning to make the accuracy of your models as high as possible. (error as low as possible)\n",
    "- Use Cross Validation techniques to get more consistent results with your model.\n",
    "- Use GridSearchCV to try different combinations of hyperparameters. \n",
    "- Start looking into other types of Keras layers for CNNs and RNNs maybe try and build a CNN model for fashion-MNIST to see how the results compare."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "LS_DS_433_Keras_Assignment.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "U4-S2-NNF (Python3)",
   "language": "python",
   "name": "u4-s2-nnf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
