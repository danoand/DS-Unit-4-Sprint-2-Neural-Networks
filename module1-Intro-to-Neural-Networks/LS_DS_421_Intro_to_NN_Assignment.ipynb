{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dVfaLrjLvxvQ"
   },
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "# Neural Networks\n",
    "\n",
    "## *Data Science Unit 4 Sprint 2 Assignment 1*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wxtoY12mwmih"
   },
   "source": [
    "## Define the Following:\n",
    "You can add image, diagrams, whatever you need to ensure that you understand the concepts below.\n",
    "\n",
    "### Input Layer:\n",
    "\n",
    "The input layer represents the data (source dataset) used to predict or generate desired outputs of an artificial neural network.  In practice this the input layer will accept rows or vectors of numerical or numerically encoded values.\n",
    "\n",
    "### Hidden Layer:\n",
    "\n",
    "A hidden layer is a set or vector of interim values that are the result of a numerical transformation of the values in our input layer. A neural network can zero or more hidden layers as input data is transformed to a set of output data values.  Hidden layers can't be accessed or manipulated directly - only through the application of the network's numerical transformations.\n",
    "\n",
    "### Output Layer:\n",
    "\n",
    "The output layer contains the results of the network process transforming inputs through the layers of the network.  \n",
    "\n",
    "### Neuron:\n",
    "\n",
    "A neuron is a node in the network that contains a numerical value.  It is the result of a transformation of a set of inputs or node values from a previous hidden layer in the network.  The transformation is typically a sum of weighted input values in addition to a bias value that is subject to an activation function.\n",
    "\n",
    "### Weight:\n",
    "\n",
    "A weight is a factor multiplied to an input in the form of a dataset vector (model input) or an interim node.  It serves as a tuning factor used to transform input data to an output that can be used as a prediction or model result. Weights are adjusted or tuned in order to minimize a cost or loss function.\n",
    "\n",
    "### Activation Function:\n",
    "\n",
    "An activation function is appled to a node's value which has been generated by transforming network inputs or inputs from previous hidden layers in network.  The activation determines whether the node is applicable as a model output or for transformation by the next hidden layer in the network (if applicable)\n",
    "\n",
    "### Node Map:\n",
    "\n",
    "A node map is visual depiction of a neural network that represents how data is transformed from model inputs to model outputs via zero or more hidden layers\n",
    "\n",
    "### Perceptron:\n",
    "\n",
    "A Perceptron is a simple neural network that takes one or more inputs and transforms those inputs as a weighted sum which is applied to an activation function resulting in a model output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NXuy9WcWzxa4"
   },
   "source": [
    "## Inputs -> Outputs\n",
    "\n",
    "### Explain the flow of information through a neural network from inputs to outputs. Be sure to include: inputs, weights, bias, and activation functions. How does it all flow from beginning to end?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PlSwIJMC0A8F"
   },
   "source": [
    "In a neural network information flows from a set of inputs and is transformed into a set of outputs.  The transformation algorithm operates in such as to minimize a cost or loss function - usually the calculation of the mean square error between predicted outputs and actual data.\n",
    "\n",
    "The transformation process comes the form of a sum of weighted inputs (sum of products) plus a bias value.  This result is applied to an activation function that determines if the value is applicable to the next step in the process (either a transformation to a subsequent hidden layer or a resultant set of output values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6sWR43PTwhSk"
   },
   "source": [
    "## Write your own perceptron code that can correctly classify (99.0% accuracy) a NAND gate. \n",
    "\n",
    "| x1 | x2 | y |\n",
    "|----|----|---|\n",
    "| 0  | 0  | 1 |\n",
    "| 1  | 0  | 1 |\n",
    "| 0  | 1  | 1 |\n",
    "| 1  | 1  | 0 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   x1  x2  y\n",
       "0   0   0  1\n",
       "1   1   0  1\n",
       "2   0   1  1\n",
       "3   1   1  0"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = { 'x1': [0,1,0,1],\n",
    "         'x2': [0,0,1,1],\n",
    "         'y':  [1,1,1,0]\n",
    "       }\n",
    "\n",
    "df = pd.DataFrame.from_dict(data).astype('int')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Sgh7VFGwnXGH"
   },
   "outputs": [],
   "source": [
    "# Create a copy of the dataframe\n",
    "df_wrk = df.copy()\n",
    "df_wrk.drop(\"y\", axis=1, inplace=True)\n",
    "\n",
    "# Generate an array of input arrays (an array of virtual data rows)\n",
    "arr_inputs = df_wrk.to_numpy()\n",
    "\n",
    "# Generate an array of outputs (targets)\n",
    "df_wrk = df.copy()\n",
    "df_wrk.drop([\"x1\", \"x2\"], axis=1, inplace=True)\n",
    "\n",
    "arr_outputs = df_wrk.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0],\n",
       "       [1, 0],\n",
       "       [0, 1],\n",
       "       [1, 1]])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0]])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate an initial set of weights to begin network processing\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "random.seed(33)\n",
    "\n",
    "# gen_weights is a function that generates initial weights for our neural network processing\n",
    "def gen_weights():\n",
    "    ret_wgts = []\n",
    "    \n",
    "    for i in range(2):\n",
    "        ret_wgts.append([random.uniform(-1.0, 1.0)])\n",
    "        \n",
    "    return np.array(ret_wgts) \n",
    "\n",
    "# sigmoid is a function that applies an activation (sigmoid) function to a weighted sum\n",
    "def sigmoid(x):\n",
    "    sgmd = 1 / (1 + np.exp(-x))\n",
    "    return sgmd\n",
    "\n",
    "# deriv_sigmoid calculates the derivative of sigmoid at a passed value (rate of change at point x)\n",
    "def deriv_sigmoid(x):\n",
    "    sgmd = sigmoid(x)\n",
    "    dtv  = sgmd * (1 - sgmd)\n",
    "    return dtv\n",
    "\n",
    "# general_activate is a function that applies an activation function to a numeric value\n",
    "def general_activate(val):\n",
    "    ret_arr = []\n",
    "    for v in val:\n",
    "        tmp_val = 0\n",
    "        if v > 0:\n",
    "            tmp_val = 1\n",
    "            \n",
    "        ret_arr.append(v)\n",
    "        \n",
    "    return np.array(ret_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.14065685],\n",
       "       [0.26446599]])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = gen_weights()\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2714412351626475"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias = random.uniform(.01, .33)\n",
    "bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning factor\n",
    "r = .25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights after training\n",
      "[[0.15237251]\n",
      " [0.15237251]]\n",
      "Output after training\n",
      "[[0.27144124]\n",
      " [0.42381375]\n",
      " [0.42381375]\n",
      " [0.57618625]]\n"
     ]
    }
   ],
   "source": [
    "# Iterate through x instances refining the network weights \n",
    "for iteration in range(50000):\n",
    "    \n",
    "    # Weighted sum of inputs / weights\n",
    "    weighted_sum = np.dot(arr_inputs, weights)\n",
    "    \n",
    "    # Apply the activate function to enable or disable outputs\n",
    "    activated_output = general_activate(weighted_sum + bias)\n",
    "    \n",
    "    # Calculate error\n",
    "    error = arr_outputs - activated_output\n",
    "    \n",
    "    # Calculate weight adjustments\n",
    "    adjustments = error * r \n",
    "    \n",
    "    # Generate adjusted weights\n",
    "    weights += np.dot(arr_inputs.T, adjustments)\n",
    "    \n",
    "print(\"Weights after training\")\n",
    "print(weights)\n",
    "\n",
    "print(\"Output after training\")\n",
    "print(activated_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xf7sdqVs0s4x"
   },
   "source": [
    "## Implement your own Perceptron Class and use it to classify a binary dataset: \n",
    "- [The Pima Indians Diabetes dataset](https://raw.githubusercontent.com/ryanleeallred/datasets/master/diabetes.csv) \n",
    "\n",
    "You may need to search for other's implementations in order to get inspiration for your own. There are *lots* of perceptron implementations on the internet with varying levels of sophistication and complexity. Whatever your approach, make sure you understand **every** line of your implementation and what its purpose is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148             72             35        0  33.6   \n",
       "1            1       85             66             29        0  26.6   \n",
       "2            8      183             64              0        0  23.3   \n",
       "3            1       89             66             23       94  28.1   \n",
       "4            0      137             40             35      168  43.1   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                     0.627   50        1  \n",
       "1                     0.351   31        0  \n",
       "2                     0.672   32        1  \n",
       "3                     0.167   21        0  \n",
       "4                     2.288   33        1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diabetes = pd.read_csv('https://raw.githubusercontent.com/ryanleeallred/datasets/master/diabetes.csv')\n",
    "diabetes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although neural networks can handle non-normalized data, scaling or normalizing your data will improve your neural network's learning speed. Try to apply the sklearn `MinMaxScaler` or `Normalizer` to your diabetes dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, Normalizer\n",
    "\n",
    "feats = list(diabetes)[:-1]\n",
    "\n",
    "X = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-W0tiX1F1hh2"
   },
   "outputs": [],
   "source": [
    "##### Update this Class #####\n",
    "\n",
    "class Perceptron:\n",
    "    \n",
    "    def __init__(self, niter = 10):\n",
    "        self.niter = niter\n",
    "    \n",
    "    def __sigmoid(self, x):\n",
    "        return None\n",
    "    \n",
    "    def __sigmoid_derivative(self, x):\n",
    "        return None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "    \"\"\"Fit training data\n",
    "    X : Training vectors, X.shape : [#samples, #features]\n",
    "    y : Target values, y.shape : [#samples]\n",
    "    \"\"\"\n",
    "\n",
    "        # Randomly Initialize Weights\n",
    "        weights = ...\n",
    "\n",
    "        for i in range(self.niter):\n",
    "            # Weighted sum of inputs / weights\n",
    "\n",
    "            # Activate!\n",
    "\n",
    "            # Cac error\n",
    "\n",
    "            # Update the Weights\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "    \"\"\"Return class label after unit step\"\"\"\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6QR4oAW1xdyu"
   },
   "source": [
    "## Stretch Goals:\n",
    "\n",
    "- Research \"backpropagation\" to learn how weights get updated in neural networks (tomorrow's lecture). \n",
    "- Implement a multi-layer perceptron. (for non-linearly separable classes)\n",
    "- Try and implement your own backpropagation algorithm.\n",
    "- What are the pros and cons of the different activation functions? How should you decide between them for the different layers of a neural network?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7310585786300049"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid = 1 / (1 + np.exp(-1))\n",
    "sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19661193324148185"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid * (1 - sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i: -25 sigmoid: 0.999999999986112 deriv: 1.3888001859448131e-11\n",
      "i: -24 sigmoid: 0.9999999999622486 deriv: 3.775135759411388e-11\n",
      "i: -23 sigmoid: 0.9999999998973812 deriv: 1.026188023339923e-10\n",
      "i: -22 sigmoid: 0.9999999997210531 deriv: 2.7894686552833313e-10\n",
      "i: -21 sigmoid: 0.9999999992417439 deriv: 7.582561239806484e-10\n",
      "i: -20 sigmoid: 0.9999999979388463 deriv: 2.0611536879193953e-09\n",
      "i: -19 sigmoid: 0.9999999943972036 deriv: 5.6027964176199655e-09\n",
      "i: -18 sigmoid: 0.9999999847700205 deriv: 1.522997926781229e-08\n",
      "i: -17 sigmoid: 0.9999999586006244 deriv: 4.139937388042264e-08\n",
      "i: -16 sigmoid: 0.9999998874648379 deriv: 1.1253514941371313e-07\n",
      "i: -15 sigmoid: 0.999999694097773 deriv: 3.0590213341738715e-07\n",
      "i: -14 sigmoid: 0.9999991684719722 deriv: 8.315273363282313e-07\n",
      "i: -13 sigmoid: 0.999997739675702 deriv: 2.260319188887599e-06\n",
      "i: -12 sigmoid: 0.9999938558253978 deriv: 6.144136851325744e-06\n",
      "i: -11 sigmoid: 0.999983298578152 deriv: 1.670114291046157e-05\n",
      "i: -10 sigmoid: 0.9999546021312976 deriv: 4.5395807735907655e-05\n",
      "i: -9 sigmoid: 0.9998766054240137 deriv: 0.00012337934976493025\n",
      "i: -8 sigmoid: 0.9996646498695336 deriv: 0.00033523767075636815\n",
      "i: -7 sigmoid: 0.9990889488055994 deriv: 0.000910221180121784\n",
      "i: -6 sigmoid: 0.9975273768433653 deriv: 0.002466509291359931\n",
      "i: -5 sigmoid: 0.9933071490757153 deriv: 0.006648056670790033\n",
      "i: -4 sigmoid: 0.9820137900379085 deriv: 0.017662706213291107\n",
      "i: -3 sigmoid: 0.9525741268224334 deriv: 0.045176659730912\n",
      "i: -2 sigmoid: 0.8807970779778823 deriv: 0.10499358540350662\n",
      "i: -1 sigmoid: 0.7310585786300049 deriv: 0.19661193324148185\n",
      "i: 0 sigmoid: 0.5 deriv: 0.25\n",
      "i: 1 sigmoid: 0.2689414213699951 deriv: 0.19661193324148185\n",
      "i: 2 sigmoid: 0.11920292202211755 deriv: 0.1049935854035065\n",
      "i: 3 sigmoid: 0.04742587317756678 deriv: 0.04517665973091214\n",
      "i: 4 sigmoid: 0.01798620996209156 deriv: 0.017662706213291118\n",
      "i: 5 sigmoid: 0.0066928509242848554 deriv: 0.006648056670790155\n",
      "i: 6 sigmoid: 0.0024726231566347743 deriv: 0.002466509291360048\n",
      "i: 7 sigmoid: 0.0009110511944006454 deriv: 0.0009102211801218265\n",
      "i: 8 sigmoid: 0.0003353501304664781 deriv: 0.00033523767075647424\n",
      "i: 9 sigmoid: 0.00012339457598623172 deriv: 0.0001233793497648489\n",
      "i: 10 sigmoid: 4.5397868702434395e-05 deriv: 4.5395807735951673e-05\n",
      "i: 11 sigmoid: 1.670142184809518e-05 deriv: 1.6701142910603434e-05\n",
      "i: 12 sigmoid: 6.144174602214718e-06 deriv: 6.1441368513331755e-06\n",
      "i: 13 sigmoid: 2.2603242979035746e-06 deriv: 2.2603191888376427e-06\n",
      "i: 14 sigmoid: 8.315280276641321e-07 deriv: 8.315273362252713e-07\n",
      "i: 15 sigmoid: 3.059022269256247e-07 deriv: 3.059021333494523e-07\n",
      "i: 16 sigmoid: 1.12535162055095e-07 deriv: 1.1253514939093229e-07\n",
      "i: 17 sigmoid: 4.1399375473943306e-08 deriv: 4.1399373760035016e-08\n",
      "i: 18 sigmoid: 1.522997951276035e-08 deriv: 1.5229979280808073e-08\n",
      "i: 19 sigmoid: 5.602796406145941e-09 deriv: 5.6027963747546125e-09\n",
      "i: 20 sigmoid: 2.0611536181902037e-09 deriv: 2.0611536139418496e-09\n",
      "i: 21 sigmoid: 7.582560422162385e-10 deriv: 7.582560416412863e-10\n",
      "i: 22 sigmoid: 2.7894680920908113e-10 deriv: 2.789468091312698e-10\n",
      "i: 23 sigmoid: 1.0261879630648827e-10 deriv: 1.0261879629595766e-10\n",
      "i: 24 sigmoid: 3.7751345441365816e-11 deriv: 3.775134543994065e-11\n"
     ]
    }
   ],
   "source": [
    "for i in range(-25, 25):\n",
    "    sigmoid = 1 / (1 + np.exp(i))\n",
    "    print(f'i: {i} sigmoid: {sigmoid} deriv: {sigmoid * (1 - sigmoid)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "LS_DS_431_Intro_to_NN_Assignment.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
